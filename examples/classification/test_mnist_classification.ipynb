{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example MLP: Data base MNIST\n",
    "\n",
    "In this example it will show how to work the library **deepensemble**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from config_test import *\n",
    "\n",
    "from sklearn import model_selection\n",
    "from matplotlib.pyplot import *\n",
    "from theano import shared, config\n",
    "import numpy as np\n",
    "\n",
    "from deepensemble.metrics import *\n",
    "from deepensemble.utils import *\n",
    "\n",
    "\n",
    "data_input, data_target, classes_labels, name_db, desc, col_names = \\\n",
    "    load_data('MNIST original', data_home='../../test_models/data', normalize=False)\n",
    "\n",
    "# Generate testing and training sets\n",
    "input_train, input_test, target_train, target_test = \\\n",
    "    model_selection.train_test_split(data_input, data_target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters of models and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = data_input.shape[1]\n",
    "n_classes = len(classes_labels)\n",
    "\n",
    "n_output = n_classes\n",
    "n_inputs = n_features\n",
    "\n",
    "n_neurons_model = int(0.75 * (n_output + n_inputs))\n",
    "\n",
    "n_ensemble_models = 4\n",
    "fn_activation1 = ActivationFunctions.sigmoid\n",
    "fn_activation2 = ActivationFunctions.sigmoid\n",
    "\n",
    "y = get_index_label_classes(translate_target(data_target, classes_labels))\n",
    "s = ITLFunctions.silverman(shared(np.array(y))).eval()\n",
    "\n",
    "list_scores = [\n",
    "    {'fun_score': mutual_information_parzen, 'name': 'Mutual Information'},\n",
    "    {'fun_score': mutual_information_cs, 'name': 'QMI CS'},\n",
    "    {'fun_score': mutual_information_ed, 'name': 'QMI ED'}\n",
    "]\n",
    "\n",
    "print('Silverman: %0.4g' % s)\n",
    "\n",
    "batch_size = 32\n",
    "max_epoch = 500\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "update_fn=sgd\n",
    "name_update='SGD'\n",
    "\n",
    "args_train_default = {'max_epoch': max_epoch, 'batch_size': batch_size, 'early_stop': False,\n",
    "              'improvement_threshold': 0.995, 'update_sets': True, 'minibatch': True}\n",
    "\n",
    "args_train_cip = {'max_epoch': max_epoch, 'batch_size': batch_size, 'early_stop': False,\n",
    "              'improvement_threshold': 0.995, 'update_sets': True, 'minibatch': True,\n",
    "              'criterion_update_params': 'cost', 'maximization_criterion': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MLP MSE\n",
    "\n",
    "This MLP net has {{n_neurons_model}} neurons in hidden layer and the output is a vector with {{n_output}} elements that represent each class (**one hot encoding**). The cost function is **MSE** and the update funtion is **SGD** (learning rate $\\eta=${{lr}})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create MLP\n",
    "mlp = get_mlp_model(\"MLP MSE\",\n",
    "                    classification=True, classes_labels=classes_labels,\n",
    "                    n_input=n_features, n_output=n_output,\n",
    "                    n_neurons=n_neurons_model,\n",
    "                    fn_activation1=fn_activation1, fn_activation2=fn_activation2,\n",
    "                    update=update_fn, name_update=name_update,\n",
    "                    cost=mse, name_cost=\"MSE\", params_update={'learning_rate': lr})\n",
    "\n",
    "metrics_mlp = FactoryMetrics.get_metric(mlp)\n",
    "# Compile\n",
    "mlp.compile(fast=True)                    \n",
    "# training\n",
    "metrics = mlp.fit(input_train, target_train, **args_train_default)\n",
    "print(\"FINISHED!\")\n",
    "# Compute metricstrain\n",
    "metrics_mlp.append_metric(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_mlp.append_prediction(input_test, target_test, append_last_pred=True)\n",
    "metrics_mlp.plot_confusion_matrix(title='Matriz de Confusión\\nMLP MSE')\n",
    "plt.show()\n",
    "\n",
    "metrics_mlp.classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Ensemble MSE\n",
    "\n",
    "This ensemble has {{n_ensemble_models}} MLP networks, where each MLP net has {{n_neurons_model}} neurons in hidden layer and the output is a vector with {{n_output}} elements that represent each class (**one hot encoding**). The cost function is **MSE** and the update funtion is **SGD** (learning rate $\\eta=${{lr}})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Ensemble\n",
    "ensemble = get_ensemble_model(name='Ensamble MSE', classification=True, classes_labels=classes_labels,\n",
    "                              n_input=n_features, n_output=n_output,\n",
    "                              n_ensemble_models=n_ensemble_models, n_neurons_model=n_neurons_model,\n",
    "                              fn_activation1=fn_activation1, fn_activation2=fn_activation2,\n",
    "                              cost=mse, name_cost=\"MSE\",\n",
    "                              update=update_fn, name_update=name_update,\n",
    "                              params_update={'learning_rate': lr})\n",
    "\n",
    "metrics_ensemble = FactoryMetrics.get_metric(ensemble)\n",
    "# Compile\n",
    "ensemble.compile(fast=True)\n",
    "# training\n",
    "metrics = ensemble.fit(input_train, target_train, **args_train_default)\n",
    "print(\"FINISHED!\")\n",
    "# Compute metricstrain\n",
    "metrics_ensemble.append_metric(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_ensemble.append_prediction(input_test, target_test, append_last_pred=True)\n",
    "metrics_ensemble.plot_confusion_matrix(title='Matriz de Confusión\\nEnsamble MSE')\n",
    "plt.show()\n",
    "\n",
    "metrics_ensemble.classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Ensemble NCL\n",
    "\n",
    "This ensemble has {{n_ensemble_models}} MLP networks, where each MLP net has {{n_neurons_model}} neurons in hidden layer and the output is a vector with {{n_output}} elements that represent each class (**one hot encoding**). The cost function is **NCL** (Negative Correlation Learning) and the update funtion is **SGD** (learning rate $\\eta=${{lr}}). The parameter of **NCL** is $\\lambda=0.3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Ensemble NCL\n",
    "ensembleNCL = get_ensembleNCL_model(name='Ensamble NCL', classification=True, classes_labels=classes_labels,\n",
    "                                        n_input=n_features, n_output=n_output,\n",
    "                                        n_ensemble_models=n_ensemble_models, n_neurons_models=n_neurons_model,\n",
    "                                        fn_activation1=fn_activation1, fn_activation2=fn_activation2,\n",
    "                                        update=update_fn, name_update=name_update,\n",
    "                                        lamb=0.3, params_update={'learning_rate': lr})\n",
    "\n",
    "metrics_ensembleNCL = FactoryMetrics.get_metric(ensembleNCL)\n",
    "# Compile\n",
    "ensembleNCL.compile(fast=True)                 \n",
    "# training\n",
    "metrics = ensembleNCL.fit(input_train, target_train, **args_train_default)\n",
    "print(\"FINISHED!\")\n",
    "# Compute metricstrain\n",
    "metrics_ensembleNCL.append_metric(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_ensembleNCL.append_prediction(input_test, target_test, append_last_pred=True)\n",
    "metrics_ensembleNCL.plot_confusion_matrix(title='Matriz de Confusión\\nEnsamble NCL')\n",
    "plt.show()\n",
    "\n",
    "metrics_ensembleNCL.classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Ensemble CIPL\n",
    "\n",
    "This ensemble has {{n_ensemble_models}} MLP networks, where each MLP net has {{n_neurons_model}} neurons in hidden layer and the output is a vector with {{n_output}} elements that represent each class (**one hot encoding**). The cost function is **CIPL** (Cross Informartion Potential Learning) and the update funtion is **SGD** (learning rate $\\eta=-0.5$). The parameters of **CIPL** are $\\beta=0.3$ and $\\lambda=0.3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Ensemble CIP\n",
    "ensembleCIP = get_ensembleCIP_model(name='Ensamble CIPL', classification=True, classes_labels=classes_labels,\n",
    "                                    n_input=n_features, n_output=n_output,\n",
    "                                    n_ensemble_models=n_ensemble_models, n_neurons_models=n_neurons_model,\n",
    "                                    is_cip_full=False,\n",
    "                                    fn_activation1=fn_activation1, fn_activation2=fn_activation2,\n",
    "                                    dist='CS',\n",
    "                                    beta=0.3, lamb=0.3, s=s,\n",
    "                                    lsp=1.5, lsm=0.5,\n",
    "                                    bias_layer=False, mse_first_epoch=True, annealing_enable=True,\n",
    "                                    update=update_fn, name_update=name_update, type='jenssen',\n",
    "                                    params_update={'learning_rate': -0.5})\n",
    "\n",
    "metrics_ensembleCIP = FactoryMetrics.get_metric(ensembleCIP)\n",
    "# Compile\n",
    "ensembleCIP.compile(fast=False)                   \n",
    "# training\n",
    "metrics = ensembleCIP.fit(input_train, target_train, **args_train_cip)\n",
    "print(\"FINISHED!\")\n",
    "# Compute metricstrain\n",
    "metrics_ensembleCIP.append_metric(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_ensembleCIP.append_prediction(input_test, target_test, append_last_pred=True)\n",
    "metrics_ensembleCIP.plot_confusion_matrix(title='Matriz de Confusión\\nEnsamble CIPL')\n",
    "plt.show()\n",
    "\n",
    "metrics_ensembleCIP.classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Scores ans cost functions of Ensemble CIPL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_ensembleCIP.plot_scores(max_epoch=max_epoch, title='')\n",
    "metrics_ensembleCIP.plot_cost(max_epoch=max_epoch, title='')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_test_ensemble = ensemble.score(input_test, target_test)\n",
    "score_train_ensemble = ensemble.score(input_train, target_train)\n",
    "\n",
    "score_test_ensembleNCL = ensembleNCL.score(input_test, target_test)\n",
    "score_train_ensembleNCL = ensembleNCL.score(input_train, target_train)\n",
    "\n",
    "score_test_ensembleCIP = ensembleCIP.score(input_test, target_test)\n",
    "score_train_ensembleCIP = ensembleCIP.score(input_train, target_train)\n",
    "\n",
    "score_test_mlp = mlp.score(input_test, target_test)\n",
    "score_train_mlp = mlp.score(input_train, target_train)\n",
    "\n",
    "print('Score Precisión')\n",
    "print('Ensamble MSE: %f / %f' % (score_train_ensemble, score_test_ensemble))\n",
    "print('Ensamble NCL: %f / %f' % (score_train_ensembleNCL, score_test_ensembleNCL))\n",
    "print('Ensamble CIP: %f / %f' % (score_train_ensembleCIP, score_test_ensembleCIP))\n",
    "print('MLP MSE: %f / %f' % (score_train_mlp, score_test_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PDF Error for each model training\n",
    "\n",
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7), dpi=80)\n",
    "lim_y = 0.04\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "plot_pdf_error(ensemble, input_train, target_train, 'Ensamble MSE', ax, fig, lim_y=lim_y)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "plot_pdf_error(ensembleNCL, input_train, target_train, 'Ensamble NCL', ax, fig, lim_y=lim_y)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "plot_pdf_error(ensembleCIP, input_train, target_train, 'Ensamble CIP', ax, fig, lim_y=lim_y)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "plot_pdf_error(mlp, input_train, target_train, 'MLP', ax, fig, lim_y=lim_y)\n",
    "\n",
    "st = fig.suptitle(\"Función de Probabilidad (pdf) del Error\\n conjunto Entrenamiento\", fontsize=18)\n",
    "\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7), dpi=80)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "plot_pdf_error(ensemble, input_test, target_test, 'Ensamble MSE', ax, fig, lim_y=lim_y)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "plot_pdf_error(ensembleNCL, input_test, target_test, 'Ensamble NCL', ax, fig, lim_y=lim_y)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "plot_pdf_error(ensembleCIP, input_test, target_test, 'Ensamble CIP', ax, fig, lim_y=lim_y)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "plot_pdf_error(mlp, input_test, target_test, 'MLP', ax, fig, lim_y=lim_y)\n",
    "\n",
    "st = fig.suptitle(\"Función de Probabilidad (pdf) del Error\\n conjunto de prueba\", fontsize=18)\n",
    "\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Prediction with noise in input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = [n for n in np.linspace(0.01, 1, 10)]\n",
    "score_snr_cs = []\n",
    "score_snr_ed = []\n",
    "score_snr_ncl = []\n",
    "score_snr_en = []\n",
    "score_snr_mlp = []\n",
    "SNR = []\n",
    "N = input_test.shape[0]\n",
    "F = input_test.shape[1]\n",
    "mu = 0.2\n",
    "\n",
    "model_cs = get_ensemble_cip_cs(0.5, s)\n",
    "model_cs.fit(input_train, target_train, **args_train_cip)\n",
    "\n",
    "model_ed = get_ensemble_cip_ed(0.5, s)\n",
    "model_ed.fit(input_train, target_train, **args_train_cip)\n",
    "\n",
    "for n in noise:\n",
    "    ne = np.random.randn(N, F) * n + mu\n",
    "    z = input_test + ne\n",
    "    score_snr_cs.append(model_cs.score(z, target_test))\n",
    "    score_snr_ed.append(model_ed.score(z, target_test))\n",
    "    score_snr_ncl.append(ensembleNCL.score(z, target_test))\n",
    "    score_snr_en.append(ensemble.score(z, target_test))\n",
    "    score_snr_mlp.append(mlp.score(z, target_test))\n",
    "    SNR.append(np.var(input_test) / np.var(ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(7,5), dpi=80)\n",
    "plt.plot(SNR, score_snr_cs, linestyle='-', label='Ensamble CIPL CS')\n",
    "plt.plot(SNR, score_snr_ed, linestyle='--', label='Ensamble CIPL ED')\n",
    "plt.plot(SNR, score_snr_ncl, linestyle=':', label='Ensamble NCL')\n",
    "plt.plot(SNR, score_snr_en, linestyle='--', label='Ensamble MSE')\n",
    "plt.plot(SNR, score_snr_mlp, linestyle='-.', label='MLP MSE')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Precisión')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Precisión')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Diversity of Ensemble CIPL when changes parameters $\\beta$ and $\\lambda$.\n",
    "\n",
    "In this test is used CIPL with Cauchy-Schwarz and Euclidean divergency. The diversity is measured with:\n",
    "\n",
    "- Kohavi wolpert Variance.\n",
    "- Generalized Diversity.\n",
    "- Coincident Failure.\n",
    "- Entropy sk.\n",
    "- Difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ensemble_cip_cs(_param, s_sigma, fast=True):\n",
    "    ensemble = get_ensembleCIP_model(name='Ensamble CIPL CS',\n",
    "                                    n_input=n_features, n_output=n_output,\n",
    "                                    n_ensemble_models=n_ensemble_models, n_neurons_models=n_neurons_model,\n",
    "                                    classification=True,\n",
    "                                    is_cip_full=False,\n",
    "                                    classes_labels=classes_labels,\n",
    "                                    fn_activation1=fn_activation1, fn_activation2=fn_activation2,\n",
    "                                    dist='CS',\n",
    "                                    beta=_param, lamb=_param, s=s_sigma,\n",
    "                                    lsp=1.5, lsm=0.5,\n",
    "                                    lr=0.1,\n",
    "                                    bias_layer=False, mse_first_epoch=True, annealing_enable=True,\n",
    "                                    update=sgd, name_update='SGD',\n",
    "                                    params_update={'learning_rate': -0.2}\n",
    "                                    )\n",
    "\n",
    "    ensemble.compile(fast=fast)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def get_ensemble_cip_ed(_param, s_sigma, fast=True):\n",
    "    ensemble = get_ensembleCIP_model(name='Ensamble CIPL ED',\n",
    "                                    n_input=n_features, n_output=n_output,\n",
    "                                    n_ensemble_models=n_ensemble_models, n_neurons_models=n_neurons_model,\n",
    "                                    classification=True,\n",
    "                                    is_cip_full=False,\n",
    "                                    classes_labels=classes_labels,\n",
    "                                    fn_activation1=fn_activation1, fn_activation2=fn_activation2,\n",
    "                                    dist='ED',\n",
    "                                    beta=_param, lamb=_param, s=s_sigma,\n",
    "                                    lsp=1.5, lsm=0.1,\n",
    "                                    lr=0.1,\n",
    "                                    bias_layer=False, mse_first_epoch=True, annealing_enable=True,\n",
    "                                    update=sgd, name_update='SGD',\n",
    "                                    params_update={'learning_rate': -0.2}\n",
    "                                    )\n",
    "\n",
    "    ensemble.compile(fast=fast)\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [n for n in np.linspace(-1, 1, 21)]\n",
    "\n",
    "score_cs = []\n",
    "diversity_cs = []\n",
    "for p in parameters:\n",
    "    model_cs = get_ensemble_cip_cs(p, s)\n",
    "    metric = model_cs.fit(input_train, target_train, **args_train_cip)\n",
    "    score_cs.append(model_cs.score(input_test, target_test))\n",
    "    metric.append_prediction(input_test, target_test, append_last_pred=True)\n",
    "    diversity_cs.append(\n",
    "        (metric.get_diversity(metric=kohavi_wolpert_variance),\n",
    "         metric.get_diversity(metric=generalized_diversity),\n",
    "         metric.get_diversity(metric=coincident_failure),\n",
    "         metric.get_diversity(metric=entropy_sk),\n",
    "         metric.get_diversity(metric=difficulty),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_ed = []\n",
    "diversity_ed = []\n",
    "for p in parameters:\n",
    "    model_ed = get_ensemble_cip_ed(p, s)\n",
    "    metric = model_ed.fit(input_train, target_train, **args_train_cip)\n",
    "    score_ed.append(model_ed.score(input_test, target_test))\n",
    "    metric.append_prediction(input_test, target_test, append_last_pred=True)\n",
    "    diversity_ed.append(\n",
    "        (metric.get_diversity(metric=kohavi_wolpert_variance),\n",
    "         metric.get_diversity(metric=generalized_diversity),\n",
    "         metric.get_diversity(metric=coincident_failure),\n",
    "         metric.get_diversity(metric=entropy_sk),\n",
    "         metric.get_diversity(metric=difficulty),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_cs = np.array(score_cs)\n",
    "diversity_cs = np.array(diversity_cs)\n",
    "score_ed = np.array(score_ed)\n",
    "diversity_ed = np.array(diversity_ed)\n",
    "\n",
    "f = plt.figure(figsize=(7,5), dpi=80)\n",
    "plt.plot(parameters, score_cs, linestyle='-', label='Ensamble CIPL CS')\n",
    "plt.plot(parameters, score_ed, linestyle='--', label='Ensamble CIPL ED')\n",
    "plt.legend()\n",
    "plt.title('Precisión')\n",
    "plt.xlabel('Parámetro')\n",
    "plt.ylabel('Precisión')\n",
    "plt.tight_layout()\n",
    "\n",
    "f = plt.figure(figsize=(7,5), dpi=80)\n",
    "plt.plot(parameters, diversity_cs[:,0, 0], linestyle='-', label='Ensamble CIPL CS')\n",
    "plt.plot(parameters, diversity_ed[:,0, 0], linestyle='--', label='Ensamble CIPL ED')\n",
    "plt.legend()\n",
    "plt.title('Diversidad Kohavi wolpert variance')\n",
    "plt.xlabel('Parámetro')\n",
    "plt.ylabel('Kohavi wolpert variance')\n",
    "plt.tight_layout()\n",
    "\n",
    "f = plt.figure(figsize=(7,5), dpi=80)\n",
    "plt.plot(parameters, diversity_cs[:,1, 0], linestyle='-', label='Ensamble CIPL CS')\n",
    "plt.plot(parameters, diversity_ed[:,1, 0], linestyle='--', label='Ensamble CIPL ED')\n",
    "plt.legend()\n",
    "plt.title('Diversidad Generalized diversity')\n",
    "plt.xlabel('Parámetro')\n",
    "plt.ylabel('Generalized diversity')\n",
    "plt.tight_layout()\n",
    "\n",
    "f = plt.figure(figsize=(7,5), dpi=80)\n",
    "plt.plot(parameters, diversity_cs[:,4, 0], linestyle='-', label='Ensamble CIPL CS')\n",
    "plt.plot(parameters, diversity_ed[:,4, 0], linestyle='--', label='Ensamble CIPL ED')\n",
    "plt.legend()\n",
    "plt.title('Diversidad Difficulty')\n",
    "plt.xlabel('Parámetro')\n",
    "plt.ylabel('Difficulty')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
