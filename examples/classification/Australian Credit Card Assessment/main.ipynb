{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Example Ensembles: Australian Credit Card Assessment\n",
    "\n",
    "In this example it will be compared 4 classifiers:\n",
    "- Multilayer perceptron.\n",
    "- Normal Ensemble.\n",
    "- Ensemble trained with Negative Correlation (Xin Yao, 1999).\n",
    "- Ensemble trained with Correntropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data\n",
    "\n",
    "This data base concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
    "  \n",
    "- Number of Instances: 690\n",
    "- Number of Attributes: 14 + class attribute\n",
    "- Attribute Information:   THERE ARE 6 NUMERICAL AND 8 CATEGORICAL ATTRIBUTES.\n",
    "- Exist 2 classes and its distribution is:\n",
    "    * +: 307 (44.5%)    CLASS 2\n",
    "    * -: 383 (55.5%)    CLASS 1\n",
    "- there are 6 numerical and 8 categorical attributes. the labels have been changed for the convenience of the statistical algorithms. For example, attribute 4 originally had 3 labels p,g,gg and these have been changed to labels 1,2,3.\n",
    "\n",
    "- Missing Attribute Values: 37 cases (5%) HAD one or more missing values. The missing values from particular attributes WERE:\n",
    "    * A1:  12\n",
    "    * A2:  12\n",
    "    * A4:   6\n",
    "    * A5:   6\n",
    "    * A6:   9\n",
    "    * A7:   9\n",
    "    * A14: 13\n",
    " \n",
    "  These were replaced by the mode of the attribute (categorical) and mean of the attribute (continuous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../..'))  # load deepensemble library\n",
    "\n",
    "from deepensemble.models.sequential import Sequential\n",
    "from deepensemble.layers.dense import Dense\n",
    "from deepensemble.utils import *\n",
    "from deepensemble.metrics import FactoryMetrics\n",
    "from deepensemble.utils.utils_functions import ActivationFunctions\n",
    "\n",
    "path_data = r'data/australian.dat'\n",
    "data = np.genfromtxt(path_data, delimiter=' ')\n",
    "\n",
    "classes_names = np.asarray(['clase 1', 'clase 2'], dtype='<U10')\n",
    "data_input    = np.asarray(data[:, 0:-1], dtype=theano.config.floatX)\n",
    "data_target   = classes_names[np.asarray(data[:, -1:], dtype=int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Experimental Setup\n",
    "\n",
    "The data set was partitioned into two sets: a training set and a testing set. The first 518 examples were used for the training set, and the remaining 172 examples for the testing set. The input attributes were rescaled to between 0.0 and 1.0 by a linear function. (**Ensemble learning via negative correlation, Y. Liua, X. Yao**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data_input_norm = preprocessing.MinMaxScaler().fit_transform(data_input)\n",
    "\n",
    "input_train = data_input_norm[0:517]\n",
    "input_test = data_input_norm[518:690]\n",
    "target_train = data_target[0:517]\n",
    "target_test = data_target[518:690]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training MLP\n",
    "\n",
    "This model has ten neurons in hidden layer and the output layer two neurons with **one hot encoding**. The cost function is MSE and update function is Stochastic gradient descendent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Compile mlp  - elapsed: 15.26 [s]\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.83[s] - left: 0.00[s] | score: 0.8571 / 0.8750\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.47[s] - left: 0.00[s] | score: 0.8795 / 0.7812\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.37[s] - left: 0.00[s] | score: 0.8594 / 0.9375\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.43[s] - left: 0.00[s] | score: 0.8638 / 0.9062\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.37[s] - left: 0.00[s] | score: 0.8571 / 0.9688\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.40[s] - left: 0.00[s] | score: 0.8549 / 0.8125\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.36[s] - left: 0.00[s] | score: 0.8594 / 0.7500\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.32[s] - left: 0.00[s] | score: 0.8594 / 0.8438\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.40[s] - left: 0.00[s] | score: 0.8571 / 0.9688\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.42[s] - left: 0.00[s] | score: 0.8616 / 0.8438\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.36[s] - left: 0.00[s] | score: 0.8750 / 0.8438\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.29[s] - left: 0.00[s] | score: 0.8638 / 0.9062\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.32[s] - left: 0.00[s] | score: 0.8616 / 0.7812\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.28[s] - left: 0.00[s] | score: 0.8817 / 0.8438\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.29[s] - left: 0.00[s] | score: 0.8705 / 0.7188\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.28[s] - left: 0.00[s] | score: 0.8571 / 0.9062\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.24[s] - left: 0.00[s] | score: 0.8638 / 0.7812\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.24[s] - left: 0.00[s] | score: 0.8594 / 0.8438\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.22[s] - left: 0.00[s] | score: 0.8616 / 0.9375\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.26[s] - left: 0.00[s] | score: 0.8616 / 0.8125\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.22[s] - left: 0.00[s] | score: 0.8728 / 0.8125\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.22[s] - left: 0.00[s] | score: 0.8683 / 0.8125\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.20[s] - left: 0.00[s] | score: 0.8772 / 0.7188\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.26[s] - left: 0.00[s] | score: 0.8638 / 0.8750\n",
      "mlp - fold: 1, epoch:[####################] 250/250 elapsed: 1.22[s] - left: 0.00[s] | score: 0.8571 / 0.9375\n",
      "FINISHED MLP!\n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential(\"mlp\", \"classifier\", classes_names)\n",
    "mlp.add_layer(Dense(n_input=data_input.shape[1], n_output=40, activation=ActivationFunctions.sigmoid))\n",
    "mlp.add_layer(Dense(n_output=len(classes_names), activation=ActivationFunctions.sigmoid))\n",
    "mlp.append_cost(mse, name='MSE')\n",
    "mlp.set_update(sgd, name='SGD',  learning_rate=0.1)\n",
    "mlp.compile(fast=False)\n",
    "\n",
    "metrics_mlp = FactoryMetrics.get_metric(mlp)\n",
    "\n",
    "max_epoch = 250\n",
    "validation_jump = 5\n",
    "\n",
    "Logger().reset()\n",
    "for i in range(25):                      \n",
    "    # training\n",
    "    metric = mlp.fit(input_train, target_train, max_epoch=max_epoch, batch_size=32, early_stop=False)\n",
    "    # Compute metrics\n",
    "    metrics_mlp.append_prediction(input_test, target_test)\n",
    "    metrics_mlp.append_metric(metric)\n",
    "    \n",
    "    # Reset parameters\n",
    "    mlp.reset()\n",
    "\n",
    "print(\"FINISHED MLP!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Results MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-425facf5a813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmetrics_mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmetrics_mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmetrics_mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cost training MLP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/backup/DeepEnsemble/deepensemble/metrics/classifiermetrics.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(self, name_report)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Precision'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f1 Score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "metrics_mlp.classification_report()\n",
    "metrics_mlp.plot_confusion_matrix()\n",
    "metrics_mlp.plot_cost(max_epoch, \"Cost training MLP\")\n",
    "metrics_mlp.plot_scores(max_epoch, \"Accuracy training MLP\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from deepensemble.models import EnsembleModel\n",
    "from deepensemble.combiner import *\n",
    "\n",
    "# Create Ensemble\n",
    "ensemble = EnsembleModel(name=\"Ensemble\")\n",
    "\n",
    "# Create models for ensemble\n",
    "for i in range(4):\n",
    "    net = Sequential(\"net%d_ens\" % i, \"classifier\", classes_names)\n",
    "    net.add_layer(Dense(n_input=data_input.shape[1], n_output=10, activation=ActivationFunctions.sigmoid))\n",
    "    net.add_layer(Dense(n_output=len(classes_names), activation=ActivationFunctions.sigmoid))\n",
    "    net.append_cost(mse, name='MSE')\n",
    "    net.set_update(sgd, name='SGD', learning_rate=0.1)\n",
    "    ensemble.append_model(net)\n",
    "\n",
    "ensemble.set_combiner(PluralityVotingCombiner())\n",
    "ensemble.compile(fast=False)\n",
    "\n",
    "metrics_ensemble = FactoryMetrics.get_metric(ensemble)\n",
    "\n",
    "Logger().reset()\n",
    "for i in range(25):                      \n",
    "    # training\n",
    "    metrics = ensemble.fit(input_train, target_train,\n",
    "                            max_epoch=max_epoch, batch_size=32, early_stop=False)\n",
    "    # Compute metrics\n",
    "    metrics_ensemble.append_prediction(input_test, target_test)\n",
    "    metrics_ensemble.append_prediction_per_model(input_test, target_test)\n",
    "    metrics_ensemble.append_metric(metrics)\n",
    "    \n",
    "    # Reset parameters of all ensemble's models\n",
    "    ensemble.reset()\n",
    "\n",
    "print(\"FINISHED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Result Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics_ensemble.classification_report()\n",
    "metrics_ensemble.diversity_report()\n",
    "metrics_ensemble.plot_confusion_matrix()\n",
    "metrics_ensemble.plot_cost(max_epoch, \"Cost training Ensemble\")\n",
    "metrics_ensemble.plot_costs(max_epoch, \"Cost training Ensemble per models\")\n",
    "metrics_ensemble.plot_scores(max_epoch, \"Accuracy training Ensemble per models\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training Ensemble - Negative Correlation (Xin Yao, 1999)\n",
    "\n",
    "The ensemble architecture used in the experiments has four networks. Each individual network is a feedforward network with one hidden layer. Both the hidden node function and the output node function are defined by the logistic function. All the individual networks have ten hidden nodes. The number of training epochs was set to 250. The strength parameter of negative correlations was set to 1.0. These parameters were chosen after limited preliminary experiments. They are not meant to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Ensemble\n",
    "ensemble_nc = EnsembleModel(name=\"Ensemble NC\")\n",
    "\n",
    "# Create models for ensemble\n",
    "for i in range(4):\n",
    "    net = Sequential(\"net%d_ens_nc\" % i, \"classifier\", classes_names)\n",
    "    net.add_layer(Dense(n_input=data_input.shape[1], n_output=10, activation=ActivationFunctions.sigmoid))\n",
    "    net.add_layer(Dense(n_output=len(classes_names), activation=ActivationFunctions.sigmoid))\n",
    "    net.append_cost(mse, name='MSE')\n",
    "    net.set_update(sgd, name='SGD', learning_rate=0.1)\n",
    "    ensemble_nc.append_model(net)\n",
    "\n",
    "ensemble_nc.add_cost_ensemble(fun_cost=neg_corr, name='NCL', lamb=1.0)  # adds neg correlation in all models\n",
    "ensemble_nc.set_combiner(PluralityVotingCombiner())\n",
    "ensemble_nc.compile(fast=False)\n",
    "\n",
    "metrics_ensemble_nc = FactoryMetrics.get_metric(ensemble_nc)\n",
    "\n",
    "Logger().reset()\n",
    "for i in range(25):                      \n",
    "    # training\n",
    "    metrics = ensemble_nc.fit(input_train, target_train,\n",
    "                                max_epoch=max_epoch, batch_size=32, early_stop=False)\n",
    "    # Compute metrics\n",
    "    metrics_ensemble_nc.append_prediction(input_test, target_test)\n",
    "    metrics_ensemble_nc.append_prediction_per_model(input_test, target_test)\n",
    "    metrics_ensemble_nc.append_metric(metrics)\n",
    "    \n",
    "    # Reset parameters of all ensemble's models\n",
    "    ensemble_nc.reset()\n",
    "\n",
    "print(\"FINISHED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results Ensemble - Negative Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics_ensemble_nc.classification_report()\n",
    "metrics_ensemble_nc.diversity_report()\n",
    "metrics_ensemble_nc.plot_confusion_matrix()\n",
    "metrics_ensemble_nc.plot_cost(max_epoch, \"Cost training Ensemble-NC\")\n",
    "metrics_ensemble_nc.plot_costs(max_epoch, \"Cost training Ensemble-NC per models\")\n",
    "metrics_ensemble_nc.plot_scores(max_epoch, \"Accuracy training Ensemble-NC per models\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training Ensemble - Correntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Ensemble\n",
    "ensemble_corrpy = EnsembleModel(name=\"Ensemble Correntropy\")\n",
    "\n",
    "# Create models for ensemble\n",
    "for i in range(4):\n",
    "    net = Sequential(\"net%d_ens_corrpy\" % i, \"classifier\", classes_names)\n",
    "    net.add_layer(Dense(n_input=data_input.shape[1], n_output=10, activation=ActivationFunctions.sigmoid))\n",
    "    net.add_layer(Dense(n_output=len(classes_names), activation=ActivationFunctions.softmax))\n",
    "    net.append_cost(kullback_leibler_generalized, name=\"KLG\")\n",
    "    net.set_update(sgd, name='SGD', learning_rate=0.001)\n",
    "    \n",
    "    net.compile()\n",
    "    net.fit(input_train, target_train, max_epoch=max_epoch, batch_size=32, early_stop=False)\n",
    "    \n",
    "    ensemble_corrpy.append_model(net)\n",
    "    \n",
    "\n",
    "ensemble_corrpy.add_cost_ensemble(neg_correntropy, name=\"NEG_CORRPY\", lamb=1.0)  # adds correntropy cost in all models\n",
    "ensemble_corrpy.set_combiner(PluralityVotingCombiner())\n",
    "ensemble_corrpy.compile(fast=False)\n",
    "\n",
    "metrics_ensemble_corrpy = FactoryMetrics.get_metric(ensemble_corrpy)\n",
    "\n",
    "Logger().reset()\n",
    "for i in range(25):                      \n",
    "    # training\n",
    "    metrics = ensemble_corrpy.fit(input_train, target_train,\n",
    "                                max_epoch=max_epoch, batch_size=32, early_stop=False)\n",
    "    # Compute metrics\n",
    "    metrics_ensemble_corrpy.append_prediction(input_test, target_test)\n",
    "    metrics_ensemble_corrpy.append_prediction_per_model(input_test, target_test)\n",
    "    metrics_ensemble_corrpy.append_metric(metrics)\n",
    "    \n",
    "    # Reset parameters of all ensemble's models\n",
    "    ensemble_corrpy.reset()\n",
    "\n",
    "print(\"FINISHED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results Ensemble - Correntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics_ensemble_corrpy.classification_report()\n",
    "metrics_ensemble_corrpy.diversity_report()\n",
    "metrics_ensemble_corrpy.plot_confusion_matrix()\n",
    "metrics_ensemble_corrpy.plot_cost(max_epoch, \"Cost training Ensemble-Correntropy\")\n",
    "metrics_ensemble_corrpy.plot_costs(max_epoch, \"Cost training Ensemble-Correntropy per models\")\n",
    "metrics_ensemble_corrpy.plot_scores(max_epoch, \"Accuracy training Ensemble-Correntropy per models\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
