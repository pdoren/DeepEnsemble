{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example Ensembles: Australian Credit Card Assessment\n",
    "\n",
    "In this example it will be compared 3 classifiers:\n",
    "- Multilayer perceptron.\n",
    "- Ensemble trained with Negative Correlation (Xin Yao, 1999).\n",
    "- Ensemble trained with Correntropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This data base concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
    "  \n",
    "- Number of Instances: 690\n",
    "- Number of Attributes: 14 + class attribute\n",
    "- Attribute Information:   THERE ARE 6 NUMERICAL AND 8 CATEGORICAL ATTRIBUTES.\n",
    "- Exist 2 classes and its distribution is:\n",
    "    * +: 307 (44.5%)    CLASS 2\n",
    "    * -: 383 (55.5%)    CLASS 1\n",
    "- there are 6 numerical and 8 categorical attributes. the labels have been changed for the convenience of the statistical algorithms. For example, attribute 4 originally had 3 labels p,g,gg and these have been changed to labels 1,2,3.\n",
    "\n",
    "- Missing Attribute Values: 37 cases (5%) HAD one or more missing values. The missing values from particular attributes WERE:\n",
    "    * A1:  12\n",
    "    * A2:  12\n",
    "    * A4:   6\n",
    "    * A5:   6\n",
    "    * A6:   9\n",
    "    * A7:   9\n",
    "    * A14: 13\n",
    " \n",
    "  These were replaced by the mode of the attribute (categorical) and mean of the attribute (continuous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 630M (CNMeM is enabled with initial size: 75.0% of memory, CuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from theano.sandbox import cuda\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "cuda.use('gpu')\n",
    "theano.config.compute_test_value = 'off'\n",
    "\n",
    "path_data = r'data\\australian.dat'\n",
    "data = np.genfromtxt(path_data, delimiter=' ')\n",
    "\n",
    "classes_names = np.asarray(['clase 1', 'clase 2'], dtype='<U10')\n",
    "data_input    = np.asarray(data[:, 0:-1], dtype=theano.config.floatX)\n",
    "data_target   = classes_names[np.asarray(data[:, -1:], dtype=int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Setup\n",
    "\n",
    "The data set was partitioned into two sets: a training set and a testing set. The first 518 examples were used for the training set, and the remaining 172 examples for the testing set. The input attributes were rescaled to between 0.0 and 1.0 by a linear function. (**Ensemble learning via negative correlation, Y. Liua, X. Yao**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data_input_norm = preprocessing.MinMaxScaler().fit_transform(data_input)\n",
    "\n",
    "input_train = data_input_norm[0:517]\n",
    "input_test = data_input_norm[518:690]\n",
    "target_train = data_target[0:517]\n",
    "target_test = data_target[518:690]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MLP\n",
    "\n",
    "This model has ten neurons in hidden layer and the output layer two neurons with **one hot encoding**. The cost function is MSE and update function is Stochastic gradient descendent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Elapsed time [s]: 3.258368\n",
      "1 Elapsed time [s]: 3.215369\n",
      "2 Elapsed time [s]: 3.260708\n",
      "3 Elapsed time [s]: 3.270795\n",
      "4 Elapsed time [s]: 3.341954\n",
      "5 Elapsed time [s]: 3.220767\n",
      "6 Elapsed time [s]: 3.230721\n",
      "7 Elapsed time [s]: 3.230868\n",
      "8 Elapsed time [s]: 3.277196\n",
      "9 Elapsed time [s]: 3.190140\n",
      "10 Elapsed time [s]: 3.210000\n",
      "11 Elapsed time [s]: 3.210279\n",
      "12 Elapsed time [s]: 3.230813\n",
      "13 Elapsed time [s]: 3.242508\n",
      "14 Elapsed time [s]: 3.230304\n",
      "15 Elapsed time [s]: 3.240257\n",
      "16 Elapsed time [s]: 3.224759\n",
      "17 Elapsed time [s]: 3.200081\n",
      "18 Elapsed time [s]: 3.240364\n",
      "19 Elapsed time [s]: 3.271020\n",
      "20 Elapsed time [s]: 3.271026\n",
      "21 Elapsed time [s]: 3.220610\n",
      "22 Elapsed time [s]: 3.281254\n",
      "23 Elapsed time [s]: 3.260726\n",
      "24 Elapsed time [s]: 3.403371\n",
      "FINISHED MLP!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from deepensemble.models.sequential import Sequential\n",
    "from deepensemble.layers.dense import Dense\n",
    "from deepensemble.utils import *\n",
    "\n",
    "mlp = Sequential(classes_names, \"classifier\", \"mlp\")\n",
    "mlp.add_layer(Dense(n_input=data_input.shape[1], n_output=10, activation=T.nnet.sigmoid))\n",
    "mlp.add_layer(Dense(n_output=len(classes_names), activation=T.nnet.sigmoid))\n",
    "mlp.append_cost(mse)\n",
    "mlp.set_update(sgd, learning_rate=0.1)\n",
    "mlp.compile()\n",
    "\n",
    "metrics_mlp = ClassifierMetrics(mlp)\n",
    "\n",
    "max_epoch = 250\n",
    "validation_jump = 5\n",
    "\n",
    "for i in range(25):                      \n",
    "    # training\n",
    "    print(\"epoch %d:\" % i, end=\"\", flush=True)\n",
    "    tic = time.time()\n",
    "    metric = mlp.fit(input_train, target_train,\n",
    "                                max_epoch=max_epoch, batch_size=32,\n",
    "                                validation_jump=validation_jump, early_stop_th=4)\n",
    "    toc = time.time()\n",
    "    print(\"Elapsed time [s]: %f\" % (toc - tic))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_mlp.append_prediction(target_test, mlp.predict(input_test))\n",
    "    metrics_mlp.append_metric(metric)\n",
    "    \n",
    "    # Reset parameters\n",
    "    mlp.reset()\n",
    "\n",
    "print(\"FINISHED MLP!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Results MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_ensemble' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1766883058bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmetrics_ensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmetrics_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmetrics_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cost training MLP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics_ensemble' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "metrics_mlp.classification_report()\n",
    "metrics_mlp.plot_confusion_matrix()\n",
    "metrics_mlp.plot_cost(max_epoch, \"Cost training MLP\")\n",
    "metrics_mlp.plot_score(max_epoch, \"Accuracy training MLP\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training Ensemble - Negative Correlation (Xin Yao, 1999)\n",
    "\n",
    "The ensemble architecture used in the experiments has four networks. Each individual network is a feedforward network with one hidden layer. Both the hidden node function and the output node function are defined by the logistic function. All the individual networks have ten hidden nodes. The number of training epochs was set to 250. The strength parameter of negative correlations was set to 1.0. These parameters were chosen after limited preliminary experiments. They are not meant to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepensemble.ensemble.ensemblemodel import EnsembleModel\n",
    "from deepensemble.combiner import *\n",
    "\n",
    "# Create Ensemble\n",
    "ensemble_nc = EnsembleModel(name=\"Ensemble NC\")\n",
    "\n",
    "# Create models for ensemble\n",
    "for i in range(4):\n",
    "    net = Sequential(classes_names, \"classifier\", \"net%d_ens_nc\" % i)\n",
    "    net.add_layer(Dense(n_input=data_input.shape[1], n_output=10, activation=T.nnet.sigmoid))\n",
    "    net.add_layer(Dense(n_output=len(classes_names), activation=T.nnet.sigmoid))\n",
    "    net.append_cost(mse)\n",
    "    net.set_update(sgd, learning_rate=0.1)\n",
    "    ensemble_nc.append_model(net)\n",
    "\n",
    "ensemble_nc.add_cost_ensemble(fun_cost=neg_corr, lamb_neg_corr=1.0)  # adds neg correlation in all models\n",
    "ensemble_nc.set_combiner(PluralityVotingCombiner())\n",
    "ensemble_nc.compile(fast=False)\n",
    "\n",
    "metrics_ensemble = EnsembleClassifierMetrics(ensemble_nc)\n",
    "\n",
    "for i in range(25):                      \n",
    "    # training\n",
    "    print(\"epoch %d:\" % i, end=\"\", flush=True)\n",
    "    tic = time.time()\n",
    "    metrics = ensemble_nc.fit(input_train, target_train,\n",
    "                                max_epoch=max_epoch, batch_size=32,\n",
    "                                validation_jump=validation_jump, early_stop_th=4)\n",
    "    toc = time.time()\n",
    "    print(\"Elapsed time [s]: %f\" % (toc - tic))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_ensemble.append_prediction(target_test, ensemble_nc.predict(input_test))\n",
    "    metrics_ensemble.append_prediction_per_model(input_test, target_test)\n",
    "    metrics_ensemble.append_metric(metrics)\n",
    "    \n",
    "    # Reset parameters of all ensemble's models\n",
    "    ensemble_nc.reset()\n",
    "\n",
    "print(\"FINISHED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Ensemble - Negative Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "metrics_ensemble.classification_report()\n",
    "metrics_ensemble.diversity_report()\n",
    "metrics_ensemble.plot_confusion_matrix()\n",
    "metrics_ensemble.plot_cost(max_epoch, \"Cost training Ensemble-NC\")\n",
    "metrics_ensemble.plot_cost_models(max_epoch, \"Cost training Ensemble-NC per models\")\n",
    "metrics_ensemble.plot_score(max_epoch, \"Accuracy training Ensemble-NC\")\n",
    "metrics_ensemble.plot_score_models(max_epoch, \"Accuracy training Ensemble-NC per models\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Ensemble - Correntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepensemble.ensemble.ensemblemodel import EnsembleModel\n",
    "\n",
    "# Create Ensemble\n",
    "ensemble_corrpy = EnsembleModel(name=\"Ensemble Correntropy\")\n",
    "\n",
    "# Create models for ensemble\n",
    "for i in range(4):\n",
    "    net = Sequential(classes_names, \"classifier\", \"net%d_ens_corrpy\" % i)\n",
    "    net.add_layer(Dense(n_input=data_input.shape[1], n_output=10, activation=T.nnet.sigmoid))\n",
    "    net.add_layer(Dense(n_output=len(classes_names), activation=T.nnet.sigmoid))\n",
    "    net.append_cost(mse)\n",
    "    net.set_update(sgd, learning_rate=0.1)\n",
    "    ensemble_corrpy.append_model(net)\n",
    "\n",
    "ensemble_corrpy.add_cost_ensemble(fun_cost=correntropy_cost, lamb_corr=1.0, s=0.1)  # adds correntropy cost in all models\n",
    "ensemble_corrpy.set_combiner(PluralityVotingCombiner())\n",
    "ensemble_corrpy.compile(fast=False)\n",
    "\n",
    "metrics_ensemble_corrpy = EnsembleClassifierMetrics(ensemble_corrpy)\n",
    "\n",
    "for i in range(25):                      \n",
    "    # training\n",
    "    print(\"epoch %d:\" % i, end=\"\", flush=True)\n",
    "    tic = time.time()\n",
    "    metrics = ensemble_corrpy.fit(input_train, target_train,\n",
    "                                max_epoch=max_epoch, batch_size=32,\n",
    "                                validation_jump=validation_jump, early_stop_th=4)\n",
    "    toc = time.time()\n",
    "    print(\"Elapsed time [s]: %f\" % (toc - tic))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_ensemble_corrpy.append_prediction(target_test, ensemble_corrpy.predict(input_test))\n",
    "    metrics_ensemble_corrpy.append_prediction_per_model(input_test, target_test)\n",
    "    metrics_ensemble_corrpy.append_metric(metrics)\n",
    "    \n",
    "    # Reset parameters of all ensemble's models\n",
    "    ensemble_corrpy.reset()\n",
    "\n",
    "print(\"FINISHED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Ensemble - Correntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "metrics_ensemble_corrpy.classification_report()\n",
    "metrics_ensemble_corrpy.diversity_report()\n",
    "metrics_ensemble_corrpy.plot_confusion_matrix()\n",
    "metrics_ensemble_corrpy.plot_cost(max_epoch, \"Cost training Ensemble-NC\")\n",
    "metrics_ensemble_corrpy.plot_cost_models(max_epoch, \"Cost training Ensemble-NC per models\")\n",
    "metrics_ensemble_corrpy.plot_score(max_epoch, \"Accuracy training Ensemble-NC\")\n",
    "metrics_ensemble_corrpy.plot_score_models(max_epoch, \"Accuracy training Ensemble-NC per models\")\n",
    "\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
